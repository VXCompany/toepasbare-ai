# Retrieval-Augmented Generation (RAG) Demo

This project demonstrates a **Retrieval-Augmented Generation (RAG)** workflow using LangChain and OpenAI's GPT models. 
The goal is to retrieve relevant informational chunks from a document and leverage a language model 
to efficiently answer questions using the retrieved context.

## Features

- **Integration with OpenAI APIs** for both `text embedding` and `language models`.
- Use of `LangChain` to construct a seamless workflow for:
  - Document loading, splitting, and embedding.
  - In-memory vector storage for similarity search.
  - Generating responses based on retrieved context.
- Pre-designed prompt templates to optimize the interaction with the language model.
- State graph setup for structuring a `retrieve` and `generate` pipeline.
- Mermaid Diagram support for visualizing workflow processes.

---

## Workflow Overview

1. **Environment Setup**:
    - API keys for OpenAI and LangSmith are configured.
    - Additional settings, such as project name and user-agent strings, are provided.

2. **Document Preparation**:
    - A document is fetched from a URL (in this case, *The Silmarillion*).
    - The text is preprocessed to extract relevant sections for further processing.
    - Processed text is split into manageable chunks for embedding and retrieval.

3. **Embedding and Vector Storage**:
    - Using OpenAI's text embedding model, document chunks are converted into vector representations.
    - These embeddings are stored in an in-memory vector store, enabling fast similarity searches.

4. **Prompting and Retrieval-Augmented Generation**:
    - A custom RAG prompt template is used to dynamically structure the interaction with the language model.
    - Relevant context chunks are retrieved for any given user question.
    - A final answer is generated by the language model based on the merged context.

5. **Pipeline Execution**:
    - A state graph is used to combine retrieval and generation into a seamless pipeline.
    - The pipeline is executed with a test question: *"What is Tolkien's message in the Silmarillion?"*.

---

## Installation

1. Clone this repository:
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Set up environment variables:
    - Define the following environment variables or input them manually at runtime:
        - `OPENAI_API_KEY` for OpenAI's API.
        - `LANGSMITH_API_KEY` for LangSmith services.

---

## Usage

1. Launch the Jupyter notebook:
    ```bash
    jupyter notebook
    ```

2. Open the provided notebook file and execute the cells step by step to:
    - Fetch and process a document.
    - Split, embed, and store text chunks in memory.
    - Run retrieval and generation to answer questions based on the document.

3. Modify the workflow or document URLs as needed for further experimentation.

---

## Example Output

The project retrieves relevant context from a provided document and generates an answer using the retrieved context. Example:

**Question**: *"What is Tolkien's message in the Silmarillion?"*  
**Answer**: (Example output from the model will appear here after execution.)

---

## Future Enhancements

- Support for additional vector stores like Elasticsearch or Pinecone.
- Improved document preprocessing for better chunking and embedding.
- Support for multimodal inputs (e.g., PDFs, images with OCR, etc.).
- Extended configuration options for dynamic prompt generation.
- Documentation on how to run this demo with a local LLM model.

---

## License

This project is licensed under the MIT License.

---

## Acknowledgments

- [LangChain](https://docs.langchain.com/) for modular and efficient language model pipeline components.
- [OpenAI](https://openai.com/) for state-of-the-art embeddings and GPT models.
- Example document: *The Silmarillion (Illustrated)* by J.R.R. Tolkien.
